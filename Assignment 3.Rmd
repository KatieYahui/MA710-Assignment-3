---
title: "MA710 - New York Times Articles - Data Science"
author: "Sevdalena Lazarova, Neha Hajela, Yahui (Katie) Zheng, Chengdong Liang"
date: "21 Mar 2017"
output:
  html_document:
    toc: yes
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
source(file="C:/Users/Chengdong Liang/Desktop/MA 710/Assignment 3/20170321_TextMining_functions.R")
```


# Introduction

This assignment uses New York Times data made available through its APIs for the purpose of encouraging innnovation through collaboration.The Article Search API announced by New York Times is a way to find, discover, explore millions of articles from 1981 onwards. Each article is comprised of searchable fields such as headline, byline, lead paragraph, publication date and Article ID among others. We request an API key that allows us to retrieve these New York Times articles. The data is returned in JSON format.

# Create the dataset

The following libraries are loaded for the purpose of text mining. Setting the width to `Inf` allows us to see all columns. The `articlesearch.key` contains the API key received from NYTimes. The `get.nyt.hits` function returns the number of hits (articles) that contain the query string "data science" between the dates `21-Mar-2016` and `21-Mar-2017`. The function `article.df` creates a dataframe of these articles called `article.df`.

```{r, message=FALSE, warning=FALSE}
library(cluster)
library(RCurl)
library(RJSONIO)
library(rlist)
library(stringr)
library(dplyr)
library(magrittr)
library(RTextTools)
library(ngram)
library(SnowballC)
options(dplyr.width=Inf)
```

 
```{r, eval= FALSE, message=FALSE, warning=FALSE}
articlesearch.key = "2cc4b704c0de4617a2e01b4719260a24"
get.nyt.hits(query.string="Oscar",     # OPTION
             begin.date="20161126",    # OPTION
             end.date  ="20170326")    # OPTION
article.df = get.nyt.articles(pages = -1, 
                              query.string = "Oscar",
                              begin.date   = "20161126",
                              end.date     = "20170426") 
save(article.df, 
     file="C:/Users/Chengdong Liang/Desktop/MA 710/Assignment 3/article.df.RData") 
```

# Load the dataset

We are loading the saved data frame every time we run or knit the document.

```{r, message=FALSE, warning=FALSE}
load(file="C:/Users/Chengdong Liang/Desktop/MA 710/Assignment 3/article.df.RData")
```
  
# Base investigation - Iteration 1


Parameter | Value | Reason
--------- | ----- | -----------------------------------------------------------
Query term | "Oscar" | Interested in exploring what were the topics related to the Oscars.
Begin date | 26 October 2017 | Interested in exploring articles posted less than 4 months from the Oscar date.
End date   | 26 March 2017 | Interested in exploring articles posted less than a month after the Oscar event.
Field      | `lead paragraph` | Gives detailed information on the main topic
Stemming   | No | We do not want to group words based on stem
N-grams    | 1 | We will start the exploration with 1 word grams
Stopwords  | "english"/"SMART" | We will use "english"
Stopwords  | "Oscar" | This is the search term.
Weighting  | term frequency | We will weightTfIdf since we would like to focus on frequency of words in document offset by frequency of a word in the whole corpus
Threshold  | 1 | We will start with threshold of 1 since the words in the articles related to Oscar have relatively low frequency values
Algorithm  | k-means | we will start with kmeans as one of the fastest and most accurate clustering algorithms we have used so far
`k`        | 6 | The first iteration will be with a random number of clusers which we decided to be equal to 6


First we are selecting only the lead paragraph from the articles which were extracted from NY Times. The reason we are selecting the lead paragraph is because it contains short and very relevant information to the topic and the content of the corresponding articles.

After that, we are cleaning the lead paragraphs documents by removing punctuation, unnecessary spacing, spaces at the end, 's at the end of the some words, dollar signs and any numbers are changed into spaces. the next step performs stemming in our case stemming is not selected as an option), then the stop words are removed. After which the function create_matrix is called which calculates the weighting of the terms. We use the tm::weightTfIdf option which basically calculated the frequency 

```{r, message=FALSE, warning=FALSE}
docs = article.df$lead_paragraph 
docs.clean = clean.documents(docs)
docs.sns = 
  modify.words(
    docs.clean,  
    stem.words=FALSE,  # OPTION: TRUE or FALSE
    ngram.vector=1, # OPTION: n-gram lengths
    stop.words=       # OPTION: stop words
      c(stopwords(kind="english")  
        # OPTION: "SMART" or "english" 
        # OPTION: additional stop words
      )
  )
doc.matrix <- 
  create_matrix(docs.sns, 
                language="english",      # Do not change
                stemWords=FALSE,         # Do not change
                removePunctuation=FALSE, # Do not change
                weighting=tm::weightTfIdf   # OPTION: weighting (see below)
  )
dtm = as.matrix(doc.matrix) 
dtm=reduce.dtm(dtm,freq.threshold=1) 
k = 6
cluster = kmeans(dtm,k)$cluster
```
### Evaluation: cluster counts 

The table below indicates the number of articles that are contained in each of our 6 clusters. 

```{r, message=FALSE, warning=FALSE, echo=FALSE}
as.data.frame(table(cluster))
```

We can see from the results, that the clusters are very unbalanced. We have cluster 1 as being with 1126 articles and cluster 3 and 6 with barely 2 and 1 articles in them. Since the results are not satisfactory we will proceed with another iteration in which we will change some of the parameters.


# Base investigation - Iteration 2

Before we perform further clustering, we will select the optimal number of clusters by using the clValid function to select the optimal number of clusters.

```{r, message=FALSE, warning=FALSE}
library(clValid)

clValid.result = clValid(dtm, nClust=2:10, clMethods=c("kmeans"), validation='internal', maxitems = nrow(dtm)) 
print(summary(clValid.result))

```

When we choose the number of clusters, we are looking to maximize Dunn (which values are from 0 to infinity), to minimize Connectivity (which values ranges from 0 to infinity) and to maximize the Silhouette (which has values between 0 to 1). Based on the results, the best option for number of clusters is 2. We will use the kmeans clustering algorithm with number of cluster 2.


Parameter | Value | Reason
--------- | ----- | -----------------------------------------------------------
Query term | "Oscar" | Interested in exploring what were the topics related to the Oscars.
Begin date | 26 October 2017 | Interested in exploring articles posted at less than 4 months from the Oscar date.
End date   | 26 March 2017 | Interested in exploring articles posted less than a month after the Oscar event.
Field      | `lead paragraph` | Gives detailed information on the main topic
Stemming   | No | We want to be able to group words based on stem
N-grams    | 1 | We will start the exploration with 1 word grams
Stopwords  | "english"/"SMART" | We will use "english"
Stopwords  | "Oscar" | This is the search term.
Weighting  | term frequency | We will weightTfIdf since we would like to focus on frequency of words in document offset by frequency of a word in the whole corpus
Threshold  | 1 | We will start with threshold of 1 since the words in the articles related to Oscar have relatively low frequency values
Algorithm  | k-means | We will start with kmeans as one of the fastest and most accurate clustering algorithms we have used so far
`k`        | 2 | The first iteration will be with a random number of clusers which we decided to be equal to 6


Compared to the first iteration, we are changing only the number of clusters - in this case we will have two clusters instead of the initial 6. 

```{r, message=FALSE, warning=FALSE}
docs = article.df$lead_paragraph 
docs.clean = clean.documents(docs)
docs.sns = 
  modify.words(
    docs.clean,  
    stem.words=FALSE,  # OPTION: TRUE or FALSE
    ngram.vector=1, # OPTION: n-gram lengths
    stop.words=       # OPTION: stop words
      c(stopwords(kind="english")  
        # OPTION: "SMART" or "english" 
        # OPTION: additional stop words
      )
  )
doc.matrix <- 
  create_matrix(docs.sns, 
                language="english",      # Do not change
                stemWords=FALSE,         # Do not change
                removePunctuation=FALSE, # Do not change
                weighting=tm::weightTfIdf   # OPTION: weighting (see below)
  )
dtm = as.matrix(doc.matrix) 
dtm=reduce.dtm(dtm,freq.threshold=1) 
k = 2
set.seed(123)
cluster = kmeans(dtm,k)$cluster
```

### Evaluation: cluster counts 

The table below indicates the number of articles that are contained in each cluster. 
```{r, echo=FALSE, message=FALSE, warning=FALSE}
as.data.frame(table(cluster))
#  17  45   2   9   2  60 839  27   5   4 
```

Again we can see that the clusters are very unbalanced. We have an enormous cluster 1 which contains 1207 of the articles and cluster 2 which contains barely 3 articles.

Since this was the best number of clusters based on the results from the clValid function, we will explore further what are the clusters common words and top words characteristics.

### Evaluation: common words 

We will explore the most common words in each of the clusters and whether we can determine a general topic for the given clusters.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
check.clusters(cluster, 1) 
```

Based on the results, we can see that cluster 1 is a mix of various articles without a common theme to them. While cluster 2 focuses on articles which focus on "mood", "party", "Cheng" and "Hollywood".

As we initially noticed, the first cluster is too general and contains a significant number of articles, while the second cluster is more focused and combines articles which revolve around the Oscars party, Hollywood, the overall mood of the party etc.

We can perform a more focused exploration of the top words in each clusters, however we expect the results are similar to what we have already observed.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# The `TopWords` function displays the documents 
# in the cluster whose number is specified with 
# third parameter
TopWords(dtm, cluster, 1) 
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# The `TopWords` function displays the documents 
# in the cluster whose number is specified with 
# third parameter
TopWords(dtm, cluster, 2) 
```

Again we can see the more focused topics of the articles in Cluster 1 which relate to the Oscar party and overall the emotions around the Oscars while the second cluster is a mix of different topics.

### Evaluation: check documents  

We will have a look at the first 10 of the articles in Cluster 2 and we can see that the topics are really about the party, Fredy Chend and Hollywood.
 
```{r ,echo=FALSE, message=FALSE, warning=FALSE}
view.cluster(2)
```

Based on our evaluation so far and the lack of satisfactory clustering results, we will look further into forming clusters by changing the weighting method from weightTfIdf (inverse document frequency) to term frequency.
 
# Base investigation - Iteration 3

We are starting with similar parameters as in Iteration 2, however this time will use the term frequency weighting method to see whether we will be able to notice any significant difference in the results.

Parameter | Value | Reason
--------- | ----- | -----------------------------------------------------------
Query term | "Oscar" | Interested in exploring what were the topics related to the Oscars.
Begin date | 26 October 2017 | Interested in exploring articles posted at less than 4 months from the Oscar date.
End date   | 26 March 2017 | Interested in exploring articles posted less than a month after the Oscar event.
Field      | `lead paragraph` | Gives detailed information on the main topic
Stemming   | No | We want to be able to group words based on stem
N-grams    | 1 | We will start the exploration with 1 word grams
Stopwords  | "english"/"SMART" | We will use "english"
Stopwords  | "Oscar" | This is the search term.
Weighting  | term frequency | We will weightTf since we would like to focus on frequency of words
Threshold  | 1 | We will start with threshold of 1 since the words in the articles related to Oscar have relatively low frequency values
Algorithm  | k-means | We will start with kmeans as one of the fastest and most accurate clustering algorithms we have used so far
`k`        | 6 | The first iteration will be with a random number of clusers which we decided to be equal to 6


```{r, eval=TRUE, message=FALSE, warning=FALSE}
docs = article.df$lead_paragraph 
docs.clean = clean.documents(docs)
docs.sns = 
  modify.words(
    docs.clean,  
    stem.words=FALSE,  # OPTION: TRUE or FALSE
    ngram.vector=1, # OPTION: n-gram lengths
    stop.words=       # OPTION: stop words
      c(stopwords(kind="english")  
        # OPTION: "SMART" or "english" 
        # OPTION: additional stop words
      )
  )
doc.matrix <- 
  create_matrix(docs.sns, 
                language="english",      # Do not change
                stemWords=FALSE,         # Do not change
                removePunctuation=FALSE, # Do not change
                weighting=tm::weightTf   # OPTION: weighting (see below)
  )
dtm = as.matrix(doc.matrix) 
dtm=reduce.dtm(dtm,freq.threshold=1) 
```


Before we perform further clustering, we will select the optimal number of clusters by using the clValid function to select the optimal number of clusters.

```{r, message=FALSE, warning=FALSE}
library(clValid)

clValid.result = clValid(dtm, nClust=2:10, clMethods=c("kmeans"), validation='internal', maxitems = nrow(dtm)) 
print(summary(clValid.result))

```

When we choose the number of clusters, we are looking to maximize Dunn (which values are from 0 to infinity), to minimize Connectivity (which values ranges from 0 to infinity) and to maximize the Silhouette (which has values between 0 to 1). Based on the results, the best option for number of clusters is 2. We will use the kmeans clustering algorithm with number of cluster 2.


```{r, message=FALSE, warning=FALSE}
k = 2
set.seed(123)
cluster = kmeans(dtm,k)$cluster
```

### Evaluation: cluster counts 

The table below indicates the number of articles that are contained in each cluster. 
```{r, echo=FALSE, message=FALSE, warning=FALSE}
as.data.frame(table(cluster))
```

Again we can see that the clusters are relatively unbalanced. We have an enormous cluster 1 which contains 1208 of the articles and cluster 1 which contains barely 2 articles.

Since this was the best number of clusters based on the results from the clValid function, we will explore further what are the clusters' common words and top words characteristics.

### Evaluation: common words 

We will explore the most common words in each of the clusters and whether we can determine a genral topic for the given clusters.


```{r, echo=FALSE, message=FALSE, warning=FALSE}
check.clusters(cluster, 1) 
```

Based on the results, we can see that cluster 1 is a mixed of various articles without a common theme to theme. While cluster 2 focuses on articles which mention New York as a focus.

As we initially noticed, the first cluster is too general and contains a significant number of articles, while the second cluster is more focused and combines articles which revolve around President Donald Trump.

We can perform a more focused exploration of the top words in each clusters, however the results are similar to what we have already observed.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# The `TopWords` function displays the documents 
# in the cluster whose number is specified with 
# third parameter
TopWords(dtm, cluster, 1) 
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# The `TopWords` function displays the documents 
# in the cluster whose number is specified with 
# third parameter
TopWords(dtm, cluster, 2) 
```

Again we can see the more focused topics of the articles in Cluster 2 which relate to New York.


### Evaluation: check documents  

We will have a look at the first 10 of the articles in Cluster 2 and we can see that the topics are really about New York.
 
```{r, echo=FALSE, message=FALSE, warning=FALSE}
view.cluster(2)[0:10]
```

We can see that the second cluster is about Oscar de la Renta and his affairs in New york. We can conclude that our articles contain information not only about the Oscars but also about a famous designer Oscar de la Renta.

Based on the second and the third iteration, we can see that the weight we choose determines the type of the clusters being formed. In the second iteration in which weighting=tm::weightTfIdf we observed a very large first cluster and very small second cluster with only three articles describing the Oscar party and the mood of the party. With the third iteration in which we set weighting=tm::weightTf, we observed a relatively small but theme focused first cluster and very large but diverse second cluster. The theme of the first cluster was New York city and news related to the fashion company Oscar de la Renta. When the weighting was set to tm::weightTf, we can observe better results related to clustering.

# Base investigation - Iteration 4

From our earlier iterations, we decide to keep the weighting as tm::weightTf, since Iteration 3 gave us better clustering results than Iteration 2. So far all our iterations have not considered Stemming. Stemming is the process of reducing words to their base form. For example a stemming algorithm would remove the ‘-ing’ ending of the word ‘jumping’ to get ‘jump’. Thus, any two words that have the same stem but different endings are considered the same word. This process makes the root word more frequent in our term document matrix and could potentially increase its importance to the document. Therefore in this iteration, we allow stemming by setting `stem.words` = TRUE.

Parameter | Value | Reason
--------- | ----- | -----------------------------------------------------------
Query term | "Oscar" | Interested in exploring what were the topics related to the Oscars.
Begin date | 26 October 2017 | Interested in exploring articles posted at less than 4 months from the Oscar date.
End date   | 26 March 2017 | Interested in exploring articles posted less than a month after the Oscar event.
Field      | `lead paragraph` | Gives detailed information on the main topic
Stemming   | Yes | We want to be able to group words based on stem
N-grams    | 1 | We will do the exploration for 1 word gram
Stopwords  | "english"/"SMART" | We will use "english"
Stopwords  | "Oscar" | This is the search term.
Weighting  | term frequency | We will weightTf since we would like to focus on frequency of words
Threshold  | 1 | We will start with threshold of 1 since the words in the articles related to Oscar have relatively low frequency values
Algorithm  | k-means | We will start with kmeans as one of the fastest and most accurate clustering algorithms we have used so far
`k`        | 10 | The first iteration will be with a random number of clusters = 10

```{r eval=TRUE, message=FALSE, warning=FALSE}
docs = article.df$lead_paragraph 
docs.clean = clean.documents(docs)
docs.sns = 
  modify.words(
    docs.clean,  
    stem.words=TRUE,  # OPTION: TRUE or FALSE
    ngram.vector=1, # OPTION: n-gram lengths
    stop.words=       # OPTION: stop words
      c(stopwords(kind="english")  
        # OPTION: "SMART" or "english" 
        # OPTION: additional stop words
      )
  )
doc.matrix <- 
  create_matrix(docs.sns, 
                language="english",      # Do not change
                stemWords=FALSE,         # Do not change
                removePunctuation=FALSE, # Do not change
                weighting=tm::weightTf   # OPTION: weighting (see below)
  )
dtm = as.matrix(doc.matrix) 
dtm=reduce.dtm(dtm,freq.threshold=1) 
k = 10
set.seed(123)
cluster = kmeans(dtm,k)$cluster
```
  
### Evaluation: cluster counts 

The table below indicates the number of articles that are contained in each cluster. 
```{r echo=FALSE, message=FALSE, warning=FALSE}
as.data.frame(table(cluster))
```

We observe that the 10 clusters formed are fairly balanced, barring a few exceptions. Clusters 1, 5, 7 and 10 have less than 4 articles out of total of 1180 articles retrieved. On the other end is Cluster 4 which contains 731 articles. However, besides these we have Clusters 2 ,3 , 6 , 8, 9 and 10 which seem to contain an adequate proportion of articles. 

### Evaluation: common words 

Now we want to investigate the clusters with a good number of articles further. We choose to drill down further into the clusters that have a minimum of atleat 20 articles.  

```{r echo=FALSE, message=FALSE, warning=FALSE}
check.clusters(cluster, 20) 
```

We observe that Clusters 2, 3, 4, 6, 7, 8, 9 and 10 have more than 20 articles. We can also see the avaerage weighting of the most common words in each cluster. For instance, the word 'trump', 'presid' and 'donald' on an average occur 0.96, 0.91 and 0.78 times in each article.

There are a few interesting observations. Cluster 1 seems to be predominantly about President Donald Trump and the travel ban implemented by him. Cluster 2 could probably be about box offixe collections of movies over the weekend. Cluster 3 seems to contain the word 'moonlight' and Cluster 8 contains the word 'land'. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
TopWords(dtm, cluster, 1) 
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
TopWords(dtm, cluster, 2) 
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
TopWords(dtm, cluster, 3) 
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
TopWords(dtm, cluster, 8) 
```

### Evaluation: check documents  

```{r echo=FALSE, message=FALSE, warning=FALSE}
view.cluster(1)[0:1]
```

This is further confirmed by looking at these articles in Cluster 1. It does talk about President Donald Trump and the travel ban was mentioned a lot of times in the articles related to Oscar.

```{r echo=FALSE}
view.cluster(2)[0:10]
```

```{r echo=FALSE}
view.cluster(3)[0:10]
```

```{r echo=FALSE}
view.cluster(8)[0:10]
```

## No stemming

We would now like to compare our results wih no stemming. So we set `stem.words` = FALSE.

Parameter | Value | Reason
--------- | ----- | -----------------------------------------------------------
Query term | "Oscar" | Interested in exploring what were the topics related to the Oscars.
Begin date | 26 October 2017 | Interested in exploring articles posted at less than 4 months from the Oscar date.
End date   | 26 March 2017 | Interested in exploring articles posted less than a month after the Oscar event.
Field      | `lead paragraph` | Gives detailed information on the main topic
Stemming   | No | We do not want to be able to group words based on stem
N-grams    | 1 | We will do the exploration for 1 word gram
Stopwords  | "english"/"SMART" | We will use "english"
Stopwords  | "Oscar" | This is the search term.
Weighting  | term frequency | We will weightTf since we would like to focus on frequency of words
Threshold  | 1 | We will start with threshold of 1 since the words in the articles related to Oscar have relatively low frequency values
Algorithm  | k-means | We will start with kmeans as one of the fastest and most accurate clustering algorithms we have used so far
`k`        | 10 | The first iteration will be with a random number of clusters = 10

```{r eval=TRUE, message=FALSE, warning=FALSE}
docs = article.df$lead_paragraph 
docs.clean = clean.documents(docs)
docs.sns = 
  modify.words(
    docs.clean,  
    stem.words=FALSE,  # OPTION: TRUE or FALSE
    ngram.vector=1, # OPTION: n-gram lengths
    stop.words=       # OPTION: stop words
      c(stopwords(kind="english")  
        # OPTION: "SMART" or "english" 
        # OPTION: additional stop words
      )
  )
doc.matrix <- 
  create_matrix(docs.sns, 
                language="english",      # Do not change
                stemWords=FALSE,         # Do not change
                removePunctuation=FALSE, # Do not change
                weighting=tm::weightTf   # OPTION: weighting (see below)
  )
dtm = as.matrix(doc.matrix) 
dtm=reduce.dtm(dtm,freq.threshold=2) 
k = 10
set.seed(123)
cluster = kmeans(dtm,k)$cluster
```

### Evaluation: cluster counts 

The table below indicates the number of articles that are contained in each cluster. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
as.data.frame(table(cluster))
```

The clusters seem fairly balanced. Only three out of the ten clusters formed have very few articles.

### Evaluation: common words 

Again, we would drill down further into the clusters that have a minimum of atleast 20 articles.  

```{r echo=FALSE, message=FALSE, warning=FALSE}
check.clusters(cluster, 20) 
```

The clustering results without stemming are very similar to the results for stemming. Clusters 1, 2 and 8 represent the same theme as earlier.

We expected stemming to improve the results of our clustering, but we didn't notice much improvement. Infact it seemed to create more problems. This is because stemming does not recognize irregular verbs. For example, 'sunday' was stemmed to 'sundai', 'president' to 'presid', 'disney' to 'disnei', 'nominated' to 'nomin' among others. These transformations did not really help our results, but in fact made the interpretations more difficult. Therefore we believe that there is not much adavantage in performing stemming.

# Base investigation - Iteration 5

The next parameter setting we want to try is to add n-grams. Since from our previous step of base inverstigation, there are names of movies, people and cities appear as common words in the clusters. For example, "Donald" and "Trump", as well as "New" and "York". We could use n-grams to modiff the clustering result.

Parameter | Value | Reason
--------- | ----- | -----------------------------------------------------------
Query term | "Oscar" | Interested in exploring what were the topics related to the Oscars.
Begin date | 26 October 2017 | Interested in exploring articles posted at less than 4 months from the Oscar date.
End date   | 26 March 2017 | Interested in exploring articles posted less than a month after the Oscar event.
Field      | `lead paragraph` | Gives detailed information on the main topic
Stemming   | No | We don't want to be able to group words based on stem
N-grams    | 1/2/3 | We will deal with 2 and 3 word grams
Stopwords  | "english"/"SMART" | We will use "english"
Stopwords  | "Oscar" | This is the search term.
Weighting  | term frequency | We will go with weightTf since we would like to focus on frequency of words
Threshold  | 3 | We will go with 3 because a lower frequency will bring in a huge doc.matrix
Algorithm  | k-means | We will start with kmeans as one of the fastest and most accurate clustering algorithms we have used so far
`k`        | 4 | We go with 4 clusters

```{r eval=TRUE, message=FALSE, warning=FALSE}
docs = article.df$lead_paragraph 
docs.clean = clean.documents(docs)
docs.sns = 
  modify.words(
    docs.clean,  
    stem.words=FALSE,  # OPTION: TRUE or FALSE
    ngram.vector=1:3, # OPTION: n-gram lengths
    stop.words=       # OPTION: stop words
      c(stopwords(kind="english")  
        # OPTION: "SMART" or "english" 
        # OPTION: additional stop words
      )
  )

doc.matrix <- 
  create_matrix(docs.sns, 
                language="english",      # Do not change
                stemWords=FALSE,         # Do not change
                removePunctuation=FALSE, # Do not change
                weighting=tm::weightTf   # OPTION: weighting (see below)
  )

dtm = as.matrix(doc.matrix) 
dtm=reduce.dtm(dtm,freq.threshold=3) 
```

After we got the matrix, we go with k-means clustering with 4 clusters.

```{r, message=FALSE, warning=FALSE}
k = 4
set.seed(127)
cluster = kmeans(dtm,centers= k, iter.max= 100, nstart= 50)$cluster
```

### Evaluation: cluster counts 

The table below indicates the number of articles that are contained in each cluster. 
```{r echo=FALSE, message=FALSE, warning=FALSE}
as.data.frame(table(cluster))
```

From the frequency, the cluster 1 has the largest size, which contains 883 articles. However, cluster 4 only contains 58 articles. The clustering result is unbalanced.

We need to explore further with common words.

### Evaluation: common words 

We will explore the most common words in each of the clusters and whether we can determine a genral topic for the given clusters.
```{r echo=FALSE, message=FALSE, warning=FALSE}
check.clusters(cluster, 1) 
```

Cluster 1 has common words as "film", "first", "new", "day", "year". The theme of the cluster is ambiguous and we could only guess it is a collection of articles talks about the Oscars, which makes perfect sense because it it the largest clusters out of four.

Cluster 2 is pretty interesting. The 89th Academy Awards ceremony took place on February 26, 2017. The common word "Sunday" indicates the date (February 26 is a Sunday).  It talks about a hot spot of OScars 2017- "La La land" mistakenly named best picture. It is a massive mistake, after presenter Faye Dunaway awarded "La La Land" the best picture Oscar that should have gone to "Moonlight" instead. "La La Land" was predicted to be the best film (common word "best film") and to win the best picture awards by many organizations before the ceremony.  This is also a reason why the mixup is so coincidental. It is reasonable that there are 242 articles talks about this news after the Oscars.

Topic of the cluster 3 is about Oscar nominations of the year. From the common words "Oscar nominated", "best", "winning" and "hollywood", we could reasonably conclude that aritcles inside of this cluster mainly talk about Oscar nomination and people may argue which nominatied film would win the reward finally.

Cluster 4 is predominant by U.S. president Donald Trump. There are 58 records inside of the cluster and looks like our president cares about Oscars too, which is not suprising at all.

### Evaluation: check documents  

We will have a look at the first 10 of the articles in Cluster 2. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
view.cluster(2)[0:10]
```

We could find that the second cluster is truly about "La La land" mistakenly named best picture. 

And we could also take a look at the first 10 of the articles in Cluster 3:


```{r echo=FALSE, message=FALSE, warning=FALSE}
view.cluster(3)[0:10]
```

From the documents check, we could conclude that the topic of cluster 3 is the Oscar nomination of 2017. It mathches the clustering result very well.


We could conclude that the clustering method works well for our clusters. We could define the topic of each clusters by simply looking at the top words or common words of the clusters.  

# Base investigation - Iteration 6

The next parameter setting we want to adjust is the stop word to improve our clustering results. Since from our previous step of inverstigation 5, there are some words in common in each cluster. For instance, words "year", "will", "said" and "one" don't really tell the main topics of that cluster. We want to eliminate those words and investigate whether the clustering results can be further improved. 

Parameter | Value | Reason
--------- | ----- | -----------------------------------------------------------
Query term | "Oscar" | Interested in exploring what were the topics related to the Oscars.
Begin date | 26 October 2017 | Interested in exploring articles posted at less than 4 months from the Oscar date.
End date   | 26 March 2017 | Interested in exploring articles posted less than a month after the Oscar event.
Field      | `lead paragraph` | Gives detailed information on the main topic
Stemming   | No | We don't want to be able to group words based on stem
N-grams    | 1/2/3 | We will deal with 2 and 3 word grams
Stopwords  | "english"/"SMART" | Stop words "english" and "SMART" are both used
Stopwords  | "Oscar" | This is the search term.
Weighting  | term frequency | We will go with weightTf since we would like to focus on frequency of words
Threshold  | 3 | We will go with 3 because a lower frequency will bring in a huge doc.matrix
Algorithm  | k-means | We will start with kmeans as one of the fastest and most accurate clustering algorithms we have used so far
`k`        | 4 | We go with 4 clusters
```{r, message=FALSE, warning=FALSE}
docs = article.df$lead_paragraph 
docs.clean = clean.documents(docs)
docs.sns = 
  modify.words(
    docs.clean,  
    stem.words=FALSE,  # OPTION: TRUE or FALSE
    ngram.vector=1:3, # OPTION: n-gram lengths
    stop.words=       # OPTION: stop words
      c(stopwords(kind="english"), stopwords(kind="SMART")
      )
  )

doc.matrix <- 
  create_matrix(docs.sns, 
                language="english",      # Do not change
                stemWords=FALSE,         # Do not change
                removePunctuation=FALSE, # Do not change
                weighting=tm::weightTf   # OPTION: weighting (see below)
  )

dtm = as.matrix(doc.matrix) 
dtm=reduce.dtm(dtm,freq.threshold=3)
```

The four clusters are still applied and the clustering results are shown below.

```{r, message=FALSE, warning=FALSE}
k = 4
set.seed(127)
cluster = kmeans(dtm,k)$cluster
```

### Evaluation: cluster counts 

The table below indicates the number of articles that are contained in each cluster. 
```{r echo=FALSE, message=FALSE, warning=FALSE}
as.data.frame(table(cluster))
```

From the frequency, the cluster 1 has the largest size, which contains 891 articles. However, cluster 3 only contains 19 articles. The clustering result is unbalanced.

### Evaluation: common words 

We will explore the most common words in each of the clusters and whether we can determine a genral topic for the given clusters.
```{r echo=FALSE, message=FALSE, warning=FALSE}
check.clusters(cluster, 1) 
```

After adding another stopword "SMART", the frequency for top words in each cluster decreases. The word frequency in cluster 1 is less than the word frequency of the other three clusters. There is also no obvious topic in cluster 1 compared to other clusters. In cluster 2, it is obvious that the academy awards and president Donald Trump are the main topics. In cluster 3, Donald Trump and the actress Meryl Streep are the main topics. In the cluster 4, the main topic is about the winning film. The film La laland are widely talked. In a nut shell, Cluster 2 is mainly about awards,with frequency as 0.65; cluster 3 is about Donald Trump and Meryl Streep, with frequency 0.94 and 0.75; Cluster 4 is about Oscar itself, with frequency 0.99. In Cluster 1, it is more about the general topics such as president, film and hollywood. Even though Cluster 1 has the largest size, the clustering results are more interested in Cluster 2, Cluster 3 and Cluster 4. 

### Evaluation: check documents  

We will have a look at the first 10 of the articles in Cluster 2. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
view.cluster(2)[0:10]
```

Clearly, the articles mentioned Academy Awards. The topic results truley reflect the core information of articles in that cluster, which means that the clustering results are useful. 

And we could also take a look at the first 10 of the articles in Cluster 3:

```{r echo=FALSE, message=FALSE, warning=FALSE}
view.cluster(3)[0:10]
```
The articles are about the emotional comments from Meryl Streep about president Donald Trump.The meaning shares similar information with our clustering topic results. 

We can also take look at the first 10 articles in Cluster 4:
```{r echo=FALSE, message=FALSE, warning=FALSE}
view.cluster(4)[0:10]
```

The articles are about the Oscar awards and ceremony. The main topic is picked by our clustering topic result. 
All in all, the clustering topic results match the actual articles meaning. The clustering results are effective in capturing different topics about Oscar. 

# Word Cloud of each cluster

We could use common words by frequency to visualize each cluster.

Firstly, we need to construct a data frame of common word and frequency of each cluster:

```{r warning=FALSE}
cluster_matrix <- check.clusters(cluster, 1)[c(-1,-2),]
col <- c("word1", "freq1", "word2", "freq2", "word3", "freq3", "word4", "freq4")
colnames(cluster_matrix) <- col
cluster_matrix
```

We could build a word cloud based on the frequency:
We use `brewer.pal` function to create valid color schemes for th words.

### Word Cloud of cluster 1: 
```{r, message=FALSE, warning=FALSE}
library(wordcloud)
library(RColorBrewer)
pal <- brewer.pal(9, "RdYlGn")
wordcloud(cluster_matrix$word1, cluster_matrix$freq1, colors=pal , rot.per=.3)
```

### Word Cloud of cluster 2: 
```{r, message=FALSE, warning=FALSE}
library(wordcloud)
library(RColorBrewer)
wordcloud(cluster_matrix$word2, cluster_matrix$freq2, colors=pal, random.color=TRUE, rot.per=.5)
```

### Word Cloud of cluster 3: 
```{r, message=FALSE, warning=FALSE}
library(wordcloud)
library(RColorBrewer)
wordcloud(cluster_matrix$word3, cluster_matrix$freq3, colors=pal, random.color=TRUE, rot.per=.4)
```

### Word Cloud of cluster 4: 
```{r, message=FALSE, warning=FALSE}
wordcloud(cluster_matrix$word4, cluster_matrix$freq4, colors=pal, random.color=TRUE, rot.per=.5)
```

# Conclusion
Based on the clustering analysis, the main topics related to Oscar are Oscar awards and ceremony, emotional comments from Meryl Streep about president Donald Trump, and Academy Awards. The clustering results match the actual article meanings. Our clustering analysis is effective. The further research can include Oscar topics in multiple years to see whether there is any consistency in the popular topics.  
