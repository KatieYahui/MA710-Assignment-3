---
title: "MA710 - New York Times Articles - Data Science"
author: "Sevdalena Lazarova"
date: "21 Mar 2017"
output:
  html_document:
    toc: yes
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
source(file="/Users/akhoury/Desktop/MA 710/Assignment 3/20170321_TextMining_functions.R")
```


# Introduction

This assignment uses New York Times data made available through its APIs for the purpose of encouraging innnovation through collaboration.The Article Search API announced by New York Times is a way to find, discover, explore millions of articles from 1981 onwards. Each article is comprised of searchable fields such as headline, byline, lead paragraph, publication date and Article ID among others. We request an API key that allows us to retrieve these New York Times articles. The data is returned in JSON format.

# Create the dataset

The following libraries are loaded for the purpose of text mining. Setting the width to `Inf` allows us to see all columns. The `articlesearch.key` contains the API key received from NYTimes. The `get.nyt.hits` function returns the number of hits (articles) that contain the query string "data science" between the dates `21-Mar-2016` and `21-Mar-2017`. The function `article.df` creates a dataframe of these articles called `article.df`.
 
```{r eval=FALSE}
library(cluster)
library(RCurl)
library(RJSONIO)
library(rlist)
library(stringr)
library(dplyr)
library(magrittr)
library(RTextTools)
library(ngram)
library(SnowballC)
options(dplyr.width=Inf)
articlesearch.key = "80663dd1c295412e867d24982fd695f8"
get.nyt.hits(query.string="Oscar",     # OPTION
             begin.date="20161126",    # OPTION
             end.date  ="20170326")    # OPTION
article.df = get.nyt.articles(pages = -1, 
                              query.string = "Oscar",
                              begin.date   = "20161126",
                              end.date     = "20170426") 
save(article.df, 
     file="/Users/akhoury/Desktop/MA 710/article.df.RData") 
```

# Load the dataset

We are loading the saved data frame every time we run or knit the document.

```{r}
load(file="/Users/akhoury/Desktop/MA 710/article.df.RData")
```
  
# Base investigation - Iteration 1


Parameter | Value | Reason
--------- | ----- | ------
Query term | "Oscar" | Interested in exploring what were the topics related to the Oscars.
Begin date | 26 October 2017 | Interested in exploring articles posted less than 4 months from the Oscar date.
End date   | 26 March 2017 | Interested in exploring articles posted less than a month after the Oscar event.
Field      | `lead paragraph` | gives detailed information on the main topic
Stemming   | No | We do not want to group words based on stem
N-grams    | 1 | we will start the exploration with 1 word grams
Stopwords  | "english"/"SMART" | we will use "english"
Stopwords  | "Oscar" | This is the search term.
Weighting  | term frequency | We will weightTfIdf since we would like to focus on frequency of words in document offset by frequency of a word in the whole corpus
Threshold  | 1 | We will start with threshold of 1 since the words in the articles related to Oscar have relatively low frequency values
Algorithm  | k-means | we will start with kmeans as one of the fastest and most accurate clustering algorithms we have used so far
`k`        | 6 | The first iteration will be with a random number of clusers which we decided to be equal to 6


First we are selecting only the lead paragraph from the articles which were extracted from NY Times. The reason we are selecting the lead paragraph is because it contains short and very relevant information into the topic and the content of the corresponding articles.

After that, we are cleaning the lead paragraphs documents by removing punctuation, unnecessary spacing, spaces at the end, 's at the end of the some words, dollar signs and any numbers are changed into spaces. the next step performs stemming in our case stemming is not selected as an option), then the stop words are removed. After which the function create_matrix is called which calculates the weighting of the terms. We use the tm::weightTfIdf option which basically calculated the frequency 

```{r eval=TRUE}
docs = article.df$lead_paragraph 
docs.clean = clean.documents(docs)
docs.sns = 
  modify.words(
    docs.clean,  
    stem.words=FALSE,  # OPTION: TRUE or FALSE
    ngram.vector=1, # OPTION: n-gram lengths
    stop.words=       # OPTION: stop words
      c(stopwords(kind="english")  
        # OPTION: "SMART" or "english" 
        # OPTION: additional stop words
      )
  )
doc.matrix <- 
  create_matrix(docs.sns, 
                language="english",      # Do not change
                stemWords=FALSE,         # Do not change
                removePunctuation=FALSE, # Do not change
                weighting=tm::weightTfIdf   # OPTION: weighting (see below)
  )
dtm = as.matrix(doc.matrix) 
dtm=reduce.dtm(dtm,freq.threshold=1) 
k = 6
cluster = kmeans(dtm,k)$cluster
```
### Evaluation: cluster counts 

The table below indicates the number of articles that are contained in each of our 6 clusters. 

```{r echo=FALSE}
as.data.frame(table(cluster))
#  17  45   2   9   2  60 839  27   5   4 
```

We can see from the results, that the clusters are very unbalanced. We have cluster 1 as being with more than 1151 articles and cluster 4 and 6 with barely 1 and 2 articles in them. Since the results are not satisfactory we will proceed with another iteration in which we will change some of the parameters.


# Base investigation - Iteration 2

Before we perform further clustering, we will select the optimal number of clusters by using the clValid function to select the optimal number of clusters.

```{r}
library(clValid)

clValid.result = clValid(dtm, nClust=2:10, clMethods=c("kmeans"), validation='internal', maxitems = nrow(dtm)) 
print(summary(clValid.result))

```

When we choose the number of clusters, we are looking to maximize Dunn (which values are from 0 to infinity), to minimize Connectivity (which values ranges from 0 to infinity) and to maximize the Silhouette (which has values between 0 to 1). Based on the results, the best option for number of clusters is 2. We will use the kmeans clustering algorithm with number of cluster 2.


Parameter | Value | Reason
--------- | ----- | ------
Query term | "Oscar" | Interested in exploring what were the topics related to the Oscars.
Begin date | 26 October 2017 | Interested in exploring articles posted at less than 4 months from the Oscar date.
End date   | 26 March 2017 | Interested in exploring articles posted less than a month after the Oscar event.
Field      | `lead paragraph` | gives detailed information on the main topic
Stemming   | No | We want to be able to group words based on stem
N-grams    | 1 | we will start the exploration with 1 word grams
Stopwords  | "english"/"SMART" | we will use "english"
Stopwords  | "Oscar" | This is the search term.
Weighting  | term frequency | We will weightTfIdf since we would like to focus on frequency of words in document offset by frequency of a word in the whole corpus
Threshold  | 1 | We will start with threshold of 1 since the words in the articles related to Oscar have relatively low frequency values
Algorithm  | k-means | we will start with kmeans as one of the fastest and most accurate clustering algorithms we have used so far
`k`        | 2 | The first iteration will be with a random number of clusers which we decided to be equal to 6


compared to the first iteration, we are changing only the number of clusters - in this case we will have two clusters instead of the initial 6. 

```{r eval=TRUE}
docs = article.df$lead_paragraph 
docs.clean = clean.documents(docs)
docs.sns = 
  modify.words(
    docs.clean,  
    stem.words=FALSE,  # OPTION: TRUE or FALSE
    ngram.vector=1, # OPTION: n-gram lengths
    stop.words=       # OPTION: stop words
      c(stopwords(kind="english")  
        # OPTION: "SMART" or "english" 
        # OPTION: additional stop words
      )
  )
doc.matrix <- 
  create_matrix(docs.sns, 
                language="english",      # Do not change
                stemWords=FALSE,         # Do not change
                removePunctuation=FALSE, # Do not change
                weighting=tm::weightTfIdf   # OPTION: weighting (see below)
  )
dtm = as.matrix(doc.matrix) 
dtm=reduce.dtm(dtm,freq.threshold=1) 
k = 2
set.seed(123)
cluster = kmeans(dtm,k)$cluster
```

### Evaluation: cluster counts 

The table below indicates the number of articles that are contained in each cluster. 
```{r echo=FALSE}
as.data.frame(table(cluster))
#  17  45   2   9   2  60 839  27   5   4 
```

Again we can see that the clusters are very unbalanced. We have an enormous cluster 2 which contains 1194 of the articles and cluster 2 which contains barely 3 articles.

Since this was the best number of clusters based on the results from the clValid function, we will explore further what are the clusters common words and top words characteristics.

### Evaluation: common words 

We will explore the most common words in each of the clusters and whether we can determine a general topic for the given clusters.

```{r echo=FALSE, warning=FALSE}
check.clusters(cluster, 1) 
```

Based on the results, we can see that cluster 2 is a mix of various articles without a common theme to them. While cluster 1 focuses on articles which focus on "mood", "party", "Cheng and "Hollywood".

As we initially noticed, the first cluster is too general and contains a significant number of articles, while the second cluster is more focused and combines articles which revolve around the Oscars party, Hollywood, the overall mood of the party etc.

We can perform a more focused exploration of the top words in each clusters, however we expect the results are similar to what we have already observed.

```{r echo=FALSE, warning=FALSE}
# The `TopWords` function displays the documents 
# in the cluster whose number is specified with 
# third parameter
TopWords(dtm, cluster, 1) 
```

```{r echo=FALSE, warning=FALSE}
# The `TopWords` function displays the documents 
# in the cluster whose number is specified with 
# third parameter
TopWords(dtm, cluster, 2) 
```

Again we can see the more focused topics of the articles in Cluster 1 which relate to the Oscar party and overall the emotions around the Oscars while the second cluster is a mix of different topics.

### Evaluation: check documents  

We will have a look at the first 10 of the articles in Cluster 1 and we can see that the topics are really about the party, Fredy Chend and Hollywood.
 
```{r echo=FALSE}
view.cluster(1)
```

Based on our evaluation so far and the lack of satisfactory clusering results, we will look further into forming clusters by changing the weighting method from weightTfIdf (inverse document frequency) to term frequency.
 
# Base investigation - Iteration 3

We are starting with similar parameters as in Iteration 2, however this time will use the term frequency weighting method to see whether we will be able to notice any significant difference in the results.

Parameter | Value | Reason
--------- | ----- | ------
Query term | "Oscar" | Interested in exploring what were the topics related to the Oscars.
Begin date | 26 October 2017 | Interested in exploring articles posted at less than 4 months from the Oscar date.
End date   | 26 March 2017 | Interested in exploring articles posted less than a month after the Oscar event.
Field      | `lead paragraph` | gives detailed information on the main topic
Stemming   | No | We want to be able to group words based on stem
N-grams    | 1 | we will start the exploration with 1 word grams
Stopwords  | "english"/"SMART" | we will use "english"
Stopwords  | "Oscar" | This is the search term.
Weighting  | term frequency | We will weightTf since we would like to focus on frequency of words
Threshold  | 1 | We will start with threshold of 1 since the words in the articles related to Oscar have relatively low frequency values
Algorithm  | k-means | we will start with kmeans as one of the fastest and most accurate clustering algorithms we have used so far
`k`        | 6 | The first iteration will be with a random number of clusers which we decided to be equal to 6


```{r eval=TRUE}
docs = article.df$lead_paragraph 
docs.clean = clean.documents(docs)
docs.sns = 
  modify.words(
    docs.clean,  
    stem.words=FALSE,  # OPTION: TRUE or FALSE
    ngram.vector=1, # OPTION: n-gram lengths
    stop.words=       # OPTION: stop words
      c(stopwords(kind="english")  
        # OPTION: "SMART" or "english" 
        # OPTION: additional stop words
      )
  )
doc.matrix <- 
  create_matrix(docs.sns, 
                language="english",      # Do not change
                stemWords=FALSE,         # Do not change
                removePunctuation=FALSE, # Do not change
                weighting=tm::weightTf   # OPTION: weighting (see below)
  )
dtm = as.matrix(doc.matrix) 
dtm=reduce.dtm(dtm,freq.threshold=1) 
```


Before we perform further clustering, we will select the optimal number of clusters by using the clValid function to select the optimal number of clusters.

```{r}
library(clValid)

clValid.result = clValid(dtm, nClust=2:10, clMethods=c("kmeans"), validation='internal', maxitems = nrow(dtm)) 
print(summary(clValid.result))

```

When we choose the number of clusters, we are looking to maximize Dunn (which values are from 0 to infinity), to minimize Connectivity (which values ranges from 0 to infinity) and to maximize the Silhouette (which has values between 0 to 1). Based on the results, the best option for number of clusters is 2. We will use the kmeans clustering algorithm with number of cluster 2.


```{r}
k = 2
set.seed(123)
cluster = kmeans(dtm,k)$cluster
```

### Evaluation: cluster counts 

The table below indicates the number of articles that are contained in each cluster. 
```{r echo=FALSE}
as.data.frame(table(cluster))
#  17  45   2   9   2  60 839  27   5   4 
```

Again we can see that the clusters are relatively unbalanced. We have an enormous cluster 1 which contains 1083 of the articles and cluster 2 which contains barely 114 articles.

Since this was the best number of clusters based on the results from the clValid function, we will explore further what are the clusters common words and top words characteristics.

### Evaluation: common words 

We will explore the most common words in each of the clusters and whether we can determine a genral topic for the given clusters.


```{r echo=FALSE, warning=FALSE}
check.clusters(cluster, 1) 
```

Based on the results, we can see that cluster 1 is a mixed of various articles without a common theme to theme. While cluster 2 focuses on articles which mention New York as a focus.

As we initially noticed, the first cluster is too general and contains a significant number of articles, while the second cluster is more focused and combines articles which revolve around President Donald Trump.

We can perform a more focused exploration of the top words in each clusters, however the results are similar to what we have already observed.

```{r echo=FALSE, warning=FALSE}
# The `TopWords` function displays the documents 
# in the cluster whose number is specified with 
# third parameter
TopWords(dtm, cluster, 1) 
```

```{r echo=FALSE, warning=FALSE}
# The `TopWords` function displays the documents 
# in the cluster whose number is specified with 
# third parameter
TopWords(dtm, cluster, 2) 
```

Again we can see the more focused topics of the articles in Cluster 2 which relate to New York.


### Evaluation: check documents  

We will have a look at the first 10 of the articles in Cluster 2 and we can see that the topics are really about New York.
 
```{r echo=FALSE}
view.cluster(2)[0:10]
```

We can see that the second cluster is about Oscar de la Renta and his affairs in New york. We can conclude that our articles contain information not only about the Oscars but also about a famous designer Oscar de la Renta.

Based on the second and the third iteration, we can see that the weight we choose determines the type of the clusters being formed. In the second iteration in which weighting=tm::weightTfIdf we observed a very large first cluster and very small second cluster with only three articles describing the Oscar party and the mood of the party. With the third iteration in which we set weighting=tm::weightTf, we observed a relatively small but theme focused first cluster and very large but diverse second cluster. The theme of the first cluster was New York city and news related to the fashion company Oscar de la Renta. When the weighting was set to tm::weightTf, we can observe better result related to clustering.

# Base investigation - Iteration 4

From our earlier iterations, we decide to keep the weighting as tm::weightTf, since Iteration 3 gave us better clustering results than Iteration 2. So far all our iterations have not considered Stemming. Stemming is the process of reducing words to their base form. For example a stemming algorithm would remove the ‘-ing’ ending of the word ‘jumping’ to get ‘jump’. Thus, any two words that have the same stem but different endings are considered the same word. This process makes the root word more frequent in our term document matrix and could potentially increase its importance to the document. Therefore in this iteration, we allow stemming by setting `stem.words` = TRUE.

Parameter | Value | Reason
--------- | ----- | ------
Query term | "Oscar" | Interested in exploring what were the topics related to the Oscars.
Begin date | 26 October 2017 | Interested in exploring articles posted at less than 4 months from the Oscar date.
End date   | 26 March 2017 | Interested in exploring articles posted less than a month after the Oscar event.
Field      | `lead paragraph` | gives detailed information on the main topic
Stemming   | Yes | We want to be able to group words based on stem
N-grams    | 1 | we will do the exploration for 1 word gram
Stopwords  | "english"/"SMART" | we will use "english"
Stopwords  | "Oscar" | This is the search term.
Weighting  | term frequency | We will weightTf since we would like to focus on frequency of words
Threshold  | 1 | We will start with threshold of 1 since the words in the articles related to Oscar have relatively low frequency values
Algorithm  | k-means | we will start with kmeans as one of the fastest and most accurate clustering algorithms we have used so far
`k`        | 10 | The first iteration will be with a random number of clusters = 10

```{r eval=TRUE}
docs = article.df$lead_paragraph 
docs.clean = clean.documents(docs)
docs.sns = 
  modify.words(
    docs.clean,  
    stem.words=TRUE,  # OPTION: TRUE or FALSE
    ngram.vector=1, # OPTION: n-gram lengths
    stop.words=       # OPTION: stop words
      c(stopwords(kind="english")  
        # OPTION: "SMART" or "english" 
        # OPTION: additional stop words
      )
  )
doc.matrix <- 
  create_matrix(docs.sns, 
                language="english",      # Do not change
                stemWords=FALSE,         # Do not change
                removePunctuation=FALSE, # Do not change
                weighting=tm::weightTf   # OPTION: weighting (see below)
  )
dtm = as.matrix(doc.matrix) 
dtm=reduce.dtm(dtm,freq.threshold=1) 
k = 10
set.seed(123)
cluster = kmeans(dtm,k)$cluster
```
  
### Evaluation: cluster counts 

The table below indicates the number of articles that are contained in each cluster. 
```{r echo=FALSE}
as.data.frame(table(cluster))
```

We observe that the 10 clusters formed are fairly balanced, barring a few exceptions. Clusters 4, 6, 9 and 10 have only 2 articles out of total of 1180 articles retrieved. On the other end is Cluster 7 which contains 756 articles. However, besides these we have Clusters 1, 2 ,3 , 5 and 8 which seem to contain an adequate proportion of articles. 

### Evaluation: common words 

Now we want to investigate the clusters with a good number of articles further. We choose to drill down further into the clusters that have a minimum of atleat 20 articles.  

```{r echo=FALSE, warning=FALSE}
check.clusters(cluster, 20) 
```

We observe that Clusters 1, 2, 3, 5, 7 and 8 have more than 20 articles. We can also see the avaerage weighting of the most common words in each cluster. For instance, the word 'trump', 'presid' and 'donald' on an average occur 0.96, 0.91 and 0.78 times in each article.

There are a few interesting observations. Cluster 1 seems to be predominantly about President Donald Trump and the travel ban implemented by him. Cluster 2 could probably be about box offixe collections of movies over the weekend. Cluster 3 seems to contain the word 'moonlight' and Cluster 8 contains the word 'land'. 

```{r echo=FALSE, warning=FALSE}
TopWords(dtm, cluster, 1) 
```

```{r echo=FALSE, warning=FALSE}
TopWords(dtm, cluster, 2) 
```

```{r echo=FALSE, warning=FALSE}
TopWords(dtm, cluster, 3) 
```

```{r echo=FALSE, warning=FALSE}
TopWords(dtm, cluster, 8) 
```

### Evaluation: check documents  

```{r echo=FALSE}
view.cluster(1)[0:10]
```

This is further confirmed by looking at these articles in Cluster 1. It does talk about President Donald Trump and the travel ban was mentioned a lot of times in the articles related to Oscar.

```{r echo=FALSE}
view.cluster(2)[0:10]
```

```{r echo=FALSE}
view.cluster(3)[0:10]
```

```{r echo=FALSE}
view.cluster(8)[0:10]
```

## No stemming

We would now like to compare our results wih no stemming. So we set `stem.words` = FALSE.

Parameter | Value | Reason
--------- | ----- | ------
Query term | "Oscar" | Interested in exploring what were the topics related to the Oscars.
Begin date | 26 October 2017 | Interested in exploring articles posted at less than 4 months from the Oscar date.
End date   | 26 March 2017 | Interested in exploring articles posted less than a month after the Oscar event.
Field      | `lead paragraph` | gives detailed information on the main topic
Stemming   | No | We do not want to be able to group words based on stem
N-grams    | 1 | we will do the exploration for 1 word gram
Stopwords  | "english"/"SMART" | we will use "english"
Stopwords  | "Oscar" | This is the search term.
Weighting  | term frequency | We will weightTf since we would like to focus on frequency of words
Threshold  | 1 | We will start with threshold of 1 since the words in the articles related to Oscar have relatively low frequency values
Algorithm  | k-means | we will start with kmeans as one of the fastest and most accurate clustering algorithms we have used so far
`k`        | 10 | The first iteration will be with a random number of clusters = 10

```{r eval=TRUE}
docs = article.df$lead_paragraph 
docs.clean = clean.documents(docs)
docs.sns = 
  modify.words(
    docs.clean,  
    stem.words=FALSE,  # OPTION: TRUE or FALSE
    ngram.vector=1, # OPTION: n-gram lengths
    stop.words=       # OPTION: stop words
      c(stopwords(kind="english")  
        # OPTION: "SMART" or "english" 
        # OPTION: additional stop words
      )
  )
doc.matrix <- 
  create_matrix(docs.sns, 
                language="english",      # Do not change
                stemWords=FALSE,         # Do not change
                removePunctuation=FALSE, # Do not change
                weighting=tm::weightTf   # OPTION: weighting (see below)
  )
dtm = as.matrix(doc.matrix) 
dtm=reduce.dtm(dtm,freq.threshold=1) 
k = 10
set.seed(123)
cluster = kmeans(dtm,k)$cluster
```

### Evaluation: cluster counts 

The table below indicates the number of articles that are contained in each cluster. 

```{r echo=FALSE}
as.data.frame(table(cluster))
```

The clusters seem fairly balanced. Only four out of the ten clusters formed have very few articles.

### Evaluation: common words 

Again, we would drill down further into the clusters that have a minimum of atleast 20 articles.  

```{r echo=FALSE, warning=FALSE}
check.clusters(cluster, 20) 
```

The clustering results without stemming are very similar to the results for stemming. Clusters 1, 2 and 8 represent the same theme as earlier.

We expected stemming to improve the results of our clustering, but we didn't notice much improvement. Infact it seemed to create more problems. This is because stemming does not recognize irregular verbs. For example, 'sunday' was stemmed to 'sundai', 'president' to 'presid', 'disney' to 'disnei', 'nominated' to 'nomin' among others. These transformations did not really help our results, but infact made the interpretations more difficult. Therefore we believe that there is not much adavantage in performing stemming.
