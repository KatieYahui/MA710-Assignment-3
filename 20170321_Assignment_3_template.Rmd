---
title: "MA710 - New York Times Articles"
author: "[Chengdong, Katie, Neha, Sev ]"
date: "21 Mar 2017"
output:
  html_document:
    toc: yes
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
source(file="~/Downloads/20170321_TextMining_functions.R")
```


# Introduction

This assignment uses New York Times data made available through its APIs for the purpose of encouraging innnovation through collaboration.The Article Search API announced by New York Times in 2009 is a way to find, discover, explore millions of articles from 1981 onwards. Each artcle is comprised of searchable fields such as headline, byline, lead paragraph, publication date and Article ID among others. We request an API key that allows us to retrieve these New York Times articles. The data is returned in JSON format.
  
# Create the dataset

 
 
```{r eval=FALSE}
library(cluster)
library(RCurl)
library(RJSONIO)
library(rlist)
library(stringr)
library(dplyr)
library(magrittr)
library(RTextTools)
library(ngram)
options(dplyr.width=Inf)
articlesearch.key = "f1b0b7570f7370114de0df19a3722f8e:7:68259170"
get.nyt.hits(query.string="dance",     # OPTION
             begin.date="20170101",    # OPTION
             end.date  ="20170107")    # OPTION
article.df = get.nyt.articles(pages = -1, 
                              query.string = "dance",
                              begin.date   = "20170101",
                              end.date     = "20170107") 
save(article.df, 
     file="~/Downloads/article.df.RData")
```

# Load the dataset

The dataset is first loaded from New York Times API during the period from March 21 2016 to March 21 2017. Then the dataset with articles from that period of time is downloaded and stored locally. Since we want to investigate how different parameters influence our clustering results, every time we run the adjusted code, we want the number of articles and the content of the articles to be constant. Every time we adjust the parameters, the dataset is read from local drive.  

```{r}
load(file="~/Downloads/article.df.RData")
```
  
# Base investigation



Parameter | Value | Reason
--------- | ----- | ------
Query term | "data science" | Interested in data science.
Begin date | 03/21/2016 | Interested in March 2016 articles.
End date   | 03/21/2017 | Interested in March 2017 articles.
Field      | `snippet` | Kept as default
Stemming   | No | Kept as default
N-grams    | 1,2 | Kept as default
Stopwords  | "english" and "SMART" |
Stopwords  | "data science" | This is the search term.
Weighting  | binary, TF-IDF, term frequency | Kept as default
Threshold  | 2 | Kept as default
Algorithm  | k-means | Kept as default
`k`        | 3 | Kept as default

[Create your clusters here. Succinctly explain your code.] 

```{r eval=TRUE}
docs = article.df$snippet 
docs.clean = clean.documents(docs)
docs.sns = 
  modify.words(
    docs.clean,  
    stem.words=FALSE,  # OPTION: TRUE or FALSE
    ngram.vector=1:2, # OPTION: n-gram lengths
    stop.words=       # OPTION: stop words
      c(stopwords(kind="english")  
        # OPTION: "SMART" or "english" 
        # OPTION: additional stop words
      )
  )
doc.matrix <- 
  create_matrix(docs.sns, 
                language="english",      # Do not change
                stemWords=FALSE,         # Do not change
                removePunctuation=FALSE, # Do not change
                weighting=tm::weightTf   # OPTION: weighting (see below)
  )
dtm = as.matrix(doc.matrix) 
dtm=reduce.dtm(dtm,freq.threshold=2) 
k = 3
cluster = kmeans(dtm,k)$cluster
```

### Evaluation: cluster counts 

The table below indicates the number of articles 
that are contained in each cluster. 
```{r echo=FALSE}
as.data.frame(table(cluster))
#  17  45   2   9   2  60 839  27   5   4 
```
[Interpret the information from the table. 
 Indicate which clusters are large enough to investigate further.
 Explain your reasoning.]

### Evaluation: common words 

[Use the `TopWords` and/or `check.clusters` function to check for common words in 
 each cluster which in the previous section you chose to 
 investigate further. 
 For each cluster indicate which words (if any) occur often enough
 to indicate a potential subject of the documents in the cluster.
 Indicate which clusters seem to have a potential subject and 
 should be investigated further.]


```{r echo=FALSE, warning=FALSE}
check.clusters(cluster, 5) 
```

```{r echo=FALSE, warning=FALSE}
# The `TopWords` function displays the documents 
# in the cluster whose number is specified with 
# third parameter
TopWords(dtm, cluster, 1) 
```

[Explain whether the documents of this cluster have a common 
 subject and describe this subject. ]

[Explain whether the documents of this cluster have a common 
 subject and describe this subject. ]

### Evaluation: check documents  

[Use the `view.cluster` function to read the documents in each cluster 
 which you decided in the previous section should be investigated further.
 Look for a single subject common to all or most documents in the cluster.
 For each cluster indicate whether all or most of the documents in the 
 cluster share a common subject.]
 
```{r echo=FALSE}
view.cluster(1)
```

[From your evaluation, decide which parameter you will change for the next investigation.]

# Investigation 2

[Describe the option you will change from your previous investigation and why you chose it.
 Modify the following table to indicate the parameter choices you made for this investigation.
 Include the reasons you made these choices. If you did not change a parameter then simply
 copy the line from the previous table.]

Parameter | Value | Reason
--------- | ----- | ------
Query term | "data science" | Interested in modern dance.
Begin date | 03/21/2016 | Interested in 2016 articles.
End date   | 03/21/2017 | Interested in 2017 articles.
Field      | `snippet` | 
Stemming   | Yes/No | 
N-grams    | 1, 2 |
Stopwords  | "english"/"SMART" |
Stopwords  | "data science" | This is the search term.
Weighting  | binary | 
Threshold  | 2 |
`k`        | 3 |

[Evaluate the clusters you create.]

[From your evaluation, decide which parameter you will change for the next investigation.]

# Investigation ?

[Repeat this process until you are satisfied that you have found good clusters and until you are satisfied that you have tried enough options.]
  
# Conclusion

[Describe the final modifications used to create your clusters 
 and describe the clusters of the cluster group.]

[Explain why this is the best set of options/parameters and clusters.]
