---
title: "MA710 - New York Times Articles"
author: "Chengdong, Katie, Neha, Sev "
date: "21 Mar 2017"
output:
  html_document:
    toc: yes
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
source(file="~/Downloads/20170321_TextMining_functions.R")
```


# Introduction

This assignment uses New York Times data made available through its APIs for the purpose of encouraging innnovation through collaboration.The Article Search API announced by New York Times in 2009 is a way to find, discover, explore millions of articles from 1981 onwards. Each artcle is comprised of searchable fields such as headline, byline, lead paragraph, publication date and Article ID among others. We request an API key that allows us to retrieve these New York Times articles. The data is returned in JSON format.
  
# Create the dataset

The following libraries are loaded for the purpose of text mining. Setting the width to `Inf` allows us to see all columns. The `articlesearch.key` contains the API key received from NYTimes. The `get.nyt.hits` function returns the number of hits (articles) that contain the query string "data science" between the dates `21-Mar-2016` and `21-Mar-2017`. The function `article.df` creates  a dataframe of these articles called `article.df`.
 
```{r eval=FALSE}
library(cluster)
library(RCurl)
library(RJSONIO)
library(rlist)
library(stringr)
library(dplyr)
library(magrittr)
library(RTextTools)
library(ngram)
options(dplyr.width=Inf)
articlesearch.key = "f1b0b7570f7370114de0df19a3722f8e:7:68259170"
get.nyt.hits(query.string="dance",     # OPTION
             begin.date="20170101",    # OPTION
             end.date  ="20170107")    # OPTION
article.df = get.nyt.articles(pages = -1, 
                              query.string = "dance",
                              begin.date   = "20170101",
                              end.date     = "20170107") 
save(article.df, 
     file="~/Downloads/article.df.RData")
```

# Load the dataset

The dataset is first loaded from New York Times API during the period from March 21 2016 to March 21 2017. Then the dataset with articles from that period of time is downloaded and stored locally. Since we want to investigate how different parameters influence our clustering results, every time we run the adjusted code, we want the number of articles and the content of the articles to be constant. Every time we adjust the parameters, the dataset is read from local drive.  

```{r}
load(file="~/Downloads/article.df.RData")
```
  
# Base investigation



Parameter | Value | Reason
--------- | ----- | ------
Query term | "data science" | Interested in data science.
Begin date | 03/21/2016 | Interested in March 2016 articles.
End date   | 03/21/2017 | Interested in March 2017 articles.
Field      | `snippet` | Kept as default
Stemming   | No | Kept as default
N-grams    | 1,2 | Kept as default
Stopwords  | "english" and "SMART" | Kept as default
Stopwords  | "data science" | This is the search term.
Weighting  | binary, TF-IDF, term frequency | Kept as default
Threshold  | 2 | Kept as default
Algorithm  | k-means | Kept as default
`k`        | 3 | Kept as default

```{r eval=TRUE}
docs = article.df$snippet

# Check a few of the documents
docs[doc.ndx]
# These same documents will be checked below
# after other modifications to the documents

# Remove punctuation and numbers.
# OPTION: you may find it useful to
# change the cleaning procedure and
# modify the function `clean.documents`.
docs.clean = clean.documents(docs)

save(docs.clean, 
     file="/Users/chengdong/Desktop/docs.clean.RData")
load(file="/Users/chengdong/Desktop/docs.clean.RData")

# Check the cleaned documents
docs.clean[doc.ndx]

# OPTIONS: see code below
# Modify the words in the documents 
# with stemming, n-grams and stopwords
docs.sns = 
  modify.words(
    docs.clean,  
    stem.words=FALSE,  # OPTION: TRUE or FALSE
    ngram.vector=1:2, # OPTION: n-gram lengths
    stop.words=       # OPTION: stop words
      c(stopwords(kind="SMART"), stopwords(kind="english"),"data science"
        # OPTION: "SMART" or "english" 
        # OPTION: additional stop words
      )
  )

# Be careful: some stop words from the 
# stopwords function might be important 
# For example, "new"
# "new" %in% stop.words # "new york", "new england" and "new hampshire" 

# Check documents
docs.sns[doc.ndx]
###### docs.sns is a list

# OPTION: weighting, see below
# Create the document matrix
doc.matrix <- 
  create_matrix(docs.sns, 
                language="english",      # Do not change
                stemWords=FALSE,         # Do not change
                removePunctuation=FALSE, # Do not change
                weighting=tm::weightTf   # OPTION: weighting (see below)
  )
###### create_matrix: Creates an object of class DocumentTermMatrix from tm that can be used in the create_container function.

# Weighting OPTIONS:
# tm::weightTfIdf - term frequency-inverse document frequency
# tm::weightTf    - term frequency
# To use binary weighting use tm::weightTf and 
# create a "binary matrix" below.
                  
# Check the document matrix
doc.matrix

# OPTIONS: none, but this command must be run
# Create the document-term matrix
dtm = as.matrix(doc.matrix) 

# Check the matrix
dtm[1:10,1:10]
dim(dtm)
########Why need[1:10,1:10]? [1:10,1] could do that
####### Why 0 output for dtm[1,1]?
###### dtm[1,1:10]
# Check the number of words in 
# the document term matrix
colnames(dtm)
ncol(dtm)

# Check the distribution of document-word frequencies 
table(dtm)

# OPTION: create a binary matrix
# in order to use binary weighting.
# DO NOT run this code if you 
# DO NOT want to use binary weighting.
# Only use with parameter
#     weighting=tm::weightTf 
# All positive frequencies become 1,
# indicating only the presence of a word  
# in a  document. 
# Uncomment the following line to use
# this code if you decide to use it. 
# dtm[dtm>1]=1 

# This may not make much of a difference 
# as nearly all document-word frequencies 
# are equal to 1. Most duplicate words are
# stopwords, and those have been removed. 

# Check the distribution of document-word frequencies 
# if you created a binary document-word matrix above
table(dtm)

# Check the distribution of word frequencies 
table(colSums(dtm))
# This gives the distribution of word frequencies 
# for the entire collection of articles

# OPTION: frequency threshold
# Keep words from the document term matrix
# that occur at least the number of times
# indicated by the `freq.threshold` parameter 
dtm=reduce.dtm(dtm,freq.threshold=2) 

# Check the number of columns/words 
# remaining in the document-term matrix
ncol(dtm)

# OPTION: number of clusters to find
k =3

# OPTION: cluster algorithm 
cluster = kmeans(dtm,k)$cluster
```

### Evaluation: cluster counts 

The table below indicates the number of articles 
that are contained in each cluster. 
```{r echo=FALSE}
as.data.frame(table(cluster))
#  17  45   2   9   2  60 839  27   5   4 
```
[Interpret the information from the table. 
 Indicate which clusters are large enough to investigate further.
 Explain your reasoning.]

### Evaluation: common words 

```{r echo=FALSE, warning=FALSE}
check.clusters(cluster, 5) 
```

```{r echo=FALSE, warning=FALSE}
# The `TopWords` function displays the documents 
# in the cluster whose number is specified with 
# third parameter
TopWords(dtm, cluster, 1) 
```

[Explain whether the documents of this cluster have a common 
 subject and describe this subject. ]

[Explain whether the documents of this cluster have a common 
 subject and describe this subject. ]

### Evaluation: check documents  

[Use the `view.cluster` function to read the documents in each cluster 
 which you decided in the previous section should be investigated further.
 Look for a single subject common to all or most documents in the cluster.
 For each cluster indicate whether all or most of the documents in the 
 cluster share a common subject.]
 
```{r echo=FALSE}
view.cluster(1)
```

# Iteration 1

We include a few more stop words - "world","people" and "year" since these are high frequency words in most clusters.

 Parameter | Value | Reason
--------- | ----- | ------
Query term | "data science" | Interested in data science.
Begin date | 03/21/2016 | Interested in March 2016 articles.
End date   | 03/21/2017 | Interested in March 2017 articles.
Field      | `snippet` | Kept as default
Stemming   | No | Kept as default
N-grams    | 1,2 | Kept as default
Stopwords  | "english" and "SMART" | Kept as default
Stopwords  | "data science" ,"world","people","year" | This is the search term.
Weighting  | binary, TF-IDF, term frequency | Kept as default
Threshold  | 2 | Kept as default
Algorithm  | k-means | Kept as default
`k`        | 3 | Kept as default

```{r eval=TRUE}
docs.sns = 
        modify.words(
                docs.clean,  
                stem.words=FALSE,  # OPTION: TRUE or FALSE
                ngram.vector=1:2, # OPTION: n-gram lengths
                stop.words=       # OPTION: stop words
                        c(stopwords(kind="SMART"), stopwords(kind="english"), "data sience", "world", "people", "year"
                          # OPTION: "SMART" or "english" 
                          # OPTION: additional stop words
                        )
        )

# Be careful: some stop words from the 
# stopwords function might be important 
# For example, "new"
# "new" %in% stop.words # "new york", "new england" and "new hampshire" 

# Check documents
docs.sns[doc.ndx]
###### docs.sns is a list

# OPTION: weighting, see below
# Create the document matrix
doc.matrix <- 
        create_matrix(docs.sns, 
                      language="english",      # Do not change
                      stemWords=FALSE,         # Do not change
                      removePunctuation=FALSE, # Do not change
                      weighting=tm::weightTf   # OPTION: weighting (see below)
        )
###### create_matrix: Creates an object of class DocumentTermMatrix from tm that can be used in the create_container function.

# Weighting OPTIONS:
# tm::weightTfIdf - term frequency-inverse document frequency
# tm::weightTf    - term frequency
# To use binary weighting use tm::weightTf and 
# create a "binary matrix" below.

# Check the document matrix
doc.matrix

# OPTIONS: none, but this command must be run
# Create the document-term matrix
dtm = as.matrix(doc.matrix) 

# Check the matrix
dtm[1:10,1:10]
dim(dtm)
########Why need[1:10,1:10]? [1:10,1] could do that
####### Why 0 output for dtm[1,1]?
###### dtm[1,1:10]
# Check the number of words in 
# the document term matrix
colnames(dtm)
ncol(dtm)

# Check the distribution of document-word frequencies 
table(dtm)

# OPTION: create a binary matrix
# in order to use binary weighting.
# DO NOT run this code if you 
# DO NOT want to use binary weighting.
# Only use with parameter
#     weighting=tm::weightTf 
# All positive frequencies become 1,
# indicating only the presence of a word  
# in a  document. 
# Uncomment the following line to use
# this code if you decide to use it. 
# dtm[dtm>1]=1 

# This may not make much of a difference 
# as nearly all document-word frequencies 
# are equal to 1. Most duplicate words are
# stopwords, and those have been removed. 

# Check the distribution of document-word frequencies 
# if you created a binary document-word matrix above
table(dtm)

# Check the distribution of word frequencies 
table(colSums(dtm))
# This gives the distribution of word frequencies 
# for the entire collection of articles

# OPTION: frequency threshold
# Keep words from the document term matrix
# that occur at least the number of times
# indicated by the `freq.threshold` parameter 
dtm=reduce.dtm(dtm,freq.threshold=2) 

# Check the number of columns/words 
# remaining in the document-term matrix
ncol(dtm)

# OPTION: number of clusters to find
k =3

# OPTION: cluster algorithm 
cluster = kmeans(dtm,k)$cluster

# Iteration 2


  
# Conclusion

[Describe the final modifications used to create your clusters 
 and describe the clusters of the cluster group.]

[Explain why this is the best set of options/parameters and clusters.]
